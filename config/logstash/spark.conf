input{
        file{
                path => "spark_work_dir/*/*/stderr"
                type => "spark"
                start_position => "beginning"
                #sincedb_path => "/dev/null"
		sincedb_path => "work_dir/sincedb/sincedb.info"
		tags => ["spark","error"]
        }
}

filter{
        grok{
                match => {
                        "message" => "(?<logtime>\d{2}/\d{2}/\d{2} \d{2}:\d{2}:\d{2})\s+ERROR\s+%{GREEDYDATA:data}"
			"path" => "/hadoopecosystem/spark/work/(?<job_id>.*)/(?<batch_id>\d+)/"
                }
        }
}

filter {
        multiline {
                pattern => "(^[a-zA-Z.]+(?:Error|Exception): .+)|(^\s+at .+)|(^\s+... \d+ more)|(^\s*Caused by:.+)"
                what => "previous"
        }
}

filter {
        if "multiline" not in [tags] {
                drop {}
        }
}

output{
        #stdout{codec => rubydebug}
        elasticsearch {
                hosts => [elastic_host]
        }
}

